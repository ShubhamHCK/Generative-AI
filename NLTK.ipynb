{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNK/XShuQtB1UPWwgGfv2OW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShubhamHCK/Generative-AI/blob/main/NLTK.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NLTK**"
      ],
      "metadata": {
        "id": "ohaejweLBxod"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "# 5 things to do with nltk\n",
        "# tokenization\n",
        "# stopwaord removal\n",
        "# stopword removal\n",
        "# stemming\n",
        "# lemmatization\n"
      ],
      "metadata": {
        "id": "Z7ZaebJAB1_B"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Kx0jYgeB71S",
        "outputId": "5ae3ce5f-11f9-4778-ad4c-d8a126ae0b71"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenization\n",
        "from nltk.tokenize import word_tokenize\n",
        "# stopword list\n",
        "from nltk.corpus import stopwords\n",
        "#stemming\n",
        "from nltk.stem import PorterStemmer\n",
        "#lemmatization\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "Q6OhdsaWClu-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_text = \"I am learning Python programming, Python is greatest language...!!\""
      ],
      "metadata": {
        "id": "U9QSo9sPDRwx"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  step-1 : lowercase\n",
        "raw_text = raw_text.lower()\n",
        "print(\"After lowercase: \",raw_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3TnJbeZD5eF",
        "outputId": "3dd886e6-865c-4725-f4f4-9866d2c6204b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After lowercase:  i am learning python programming, python is greatest language...!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# step-2 : tokenization\n",
        "#print(raw_text.split()) it doesn't seperate commas and ..!\n",
        "token = word_tokenize(raw_text)\n",
        "print(\"After tokenization: \",token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QT7PRezBENNE",
        "outputId": "306bb0bf-aa9d-4aea-b636-f9b9a0049a52"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After tokenization:  ['i', 'am', 'learning', 'python', 'programming', ',', 'python', 'is', 'greatest', 'language', '...', '!', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# step -3 remove stopwords\n",
        "eng_stopwords = stopwords.words(\"english\")\n"
      ],
      "metadata": {
        "id": "vcFxshD6EXz_"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_token = []\n",
        "for word in token:\n",
        "  if word not in eng_stopwords:\n",
        "    filtered_token.append(word)\n",
        "print(\"After stopword removal: \",filtered_token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ncAfBGJgF1i_",
        "outputId": "8e44b4d9-c023-40b5-a8d7-dd1c56d6c407"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After stopword removal:  ['learning', 'python', 'programming', ',', 'python', 'greatest', 'language', '...', '!', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# step 4 : remove punctuation\n",
        "import string\n",
        "punc = string.punctuation\n",
        "print(\"punctuations: \",punc)\n",
        "clean_token = [word for word in filtered_token if word not in punc]\n",
        "print(\"After punctuation removal: \",clean_token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15y56le5G224",
        "outputId": "d4180a81-5b83-4cbc-c8db-d3623f277805"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "punctuations:  !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
            "After punctuation removal:  ['learning', 'python', 'programming', 'python', 'greatest', 'language', '...']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# step 5 : stemming\n",
        "stem = PorterStemmer()\n",
        "stem.stem(\"playing\")\n",
        "stem.stem(\"morning\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ivb9e5vVIG29",
        "outputId": "cfe286fc-be25-458e-8aa2-e8cb885cad5a"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'morn'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wnet = WordNetLemmatizer()\n",
        "#wnet.lemmatize(\"playing\",\"v\")\n",
        "wnet.lemmatize(\"was\",\"v\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "1BeLw3EzISyC",
        "outputId": "79885ff3-98a5-4936-9423-2aa287069bc7"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'be'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatized_words = []\n",
        "for words in clean_token:\n",
        "  lemmatized_words.append(wnet.lemmatize(words,\"v\"))\n",
        "print(\"After lemmatization: \",lemmatized_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EIv8Q5UEIhXs",
        "outputId": "51bd4a6e-d5f2-4618-ae97-3417fc161652"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After lemmatization:  ['learn', 'python', 'program', 'python', 'greatest', 'language', '...']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_tokens = []\n",
        "for word in lemmatized_words:\n",
        "  if word.isalpha():\n",
        "    final_tokens.append(word)\n",
        "print(\"Final tokens: \",final_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AYslmqhPJCad",
        "outputId": "68b524ba-d31b-4494-b7f2-f4b5b4a714f8"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final tokens:  ['learn', 'python', 'program', 'python', 'greatest', 'language']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences =[\n",
        "    \"I am learning python and python\",\n",
        "    \"we are happy today\",\n",
        "    \"I am sad today\"\n",
        "]\n",
        "def preprocess_text(documents):\n",
        "  all_tokens=[]\n",
        "  for document in documents:\n",
        "    document = document.lower()\n",
        "    tokens = word_tokenize(document)\n",
        "    filtered_token = []\n",
        "    for word in tokens:\n",
        "      if word not in eng_stopwords:\n",
        "        filtered_token.append(word)\n",
        "    punc = string.punctuation\n",
        "    clean_token = [word for word in filtered_token if word not in punc]\n",
        "    lemmatized_words = []\n",
        "    for words in clean_token:\n",
        "      lemmatized_words.append(wnet.lemmatize(words,\"v\"))\n",
        "    final_tokens = []\n",
        "    for word in lemmatized_words:\n",
        "      if word.isalpha():\n",
        "        final_tokens.append(word)\n",
        "    all_tokens.append(final_tokens)\n",
        "  return all_tokens\n",
        "\n",
        "cleaned_sentences = preprocess_text(sentences)\n",
        "print(cleaned_sentences)\n",
        "# expected output\n",
        "# \"learn python python\"\n",
        "# \"happy today\"\n",
        "# \"sad today\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NYSjj6GSJ6e3",
        "outputId": "040b356e-63e0-41db-e4ab-345854abba10"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['lean', 'python', 'python'], ['happy', 'today'], ['sad', 'today']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mmTvTAzgOzxK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}